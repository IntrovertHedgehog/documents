---
title: "8-1 Simple Linear Regression using R"
output: pdf_document
---

This markdown file contains the R codes for the Video: **8-1 Simple Linear Regression using R**. We have shown the slide number associated with each chunk of code for easy reference.

In this video, we will work with the `Advertising` data set. Before proceeding, place the data set `Advertising.csv` in the `data` folder. This R markdown file itself, `8-1_Simple_Linear_Regression.rmd`, should be placed in `src` folder.

## Load packages

We load the packages using the `library()` function. Ensure that the packages are installed before doing this.

```{r include = FALSE}
library(tidyverse) # for data manipulations and visualisations
library(e1071) # for skewness 
library(stats) 
```

## Slide 10

The data is in the CSV (comma separated values) format. We will read it in using the `read.csv()` function. The input parameter to the `read.csv()` function is the location of the file relative to the current working directory. This is called the *file path*. Notice, the `..` in the file path. This represents the parent directory. The `read.csv()` function returns the dataset in the form of a data frame, and is stored in the variable `df.advert`.

```{r}
df.advert <- read.csv(file = "../data/Advertising.csv")
```

Next, view the top few rows in `df.advert` using the `head()` function.

```{r}
head(df.advert)
```

## Slide 11

Let us check for any missing values in this dataset. The `is.na()` function is used here on the whole dataset. The function returns a matrix of values that are `TRUE` if that corresponding value in the dataset is missing and `FALSE`, otherwise. Then, we count all the `TRUE` values using the `sum()` function. If there are no missing values, then this sum should be zero!

```{r}
sum(is.na(df.advert))
```

Next, let us check if there are duplicate values in the dataset with the `duplicated()` function. This returns a `TRUE` for an observation (i.e. a row in the dataset), if it is a duplicate, and `FALSE`, otherwise. As before the function is used to count all the duplicated observations.

```{r}
sum(duplicated(df.advert)) 

```

We also note the total number of observations in the dataset, using the `nrow()` function. In this dataset, each row represents a city.

```{r}
nrow(df.advert)
```

## Slide 12

We will now check the distribution of each variable in the dataset. We can plot a histogram to visualise distribution of `Sales.Revenue`. These are the steps to generate the histogram:

1.  We use the `ggplot()` function for this plot. The dataset in `df.advert` is passed to the input parameter `data`.

2.  Next, we need to setup the `mapping` parameter. The histogram only requires an `x` aesthetic and we assign `Sales.Revenue` to the input parameter `x` in the `aes()` function.

3.  Add the `geom` layer to the plot. Since we want a histogram, we will add the `geom_histogram()` function.

4.  Update the axis labels for the y and x axes, and the title of the plot, using the `labs()` function.

```{r}
p.hist.Revenue <-
  ggplot(data = df.advert, mapping = aes(x = Sales.Revenue)) +
  geom_histogram() +
  labs(y = "Number of Cities",
       x = "Sales Revenue",
       title = "Distribution of Sales Revenue Across 200 Different Cities")
p.hist.Revenue
```

Next, we generate the box plot for `Sales.Revenue`. These are the steps:

1.  As before, we use the `ggplot()` function for this plot. The dataset in `df.advert` is passed to the input parameter `data`.
2.  The whiskers of the boxplot are plotted using the `stat_boxplot()` function.
3.  Add the `geom` layer to the plot. Since this is a boxplot, we will add the `geom_boxplot()` function.
4.  Update the axis labels for the x axis using the `labs()` function.
5.  Remove the text on y axis using the `theme()` function.

```{r}
p.boxplot.Revenue <- 
  ggplot(data = df.advert, aes(x = Sales.Revenue)) +
  stat_boxplot(geom = "errorbar", width = 0.2) +
  geom_boxplot() +
  labs(x = "Sales Revenue") +
  theme(axis.text.y=element_blank())
p.boxplot.Revenue
```

Using similar codes, we will plot the histogram and boxplot for `Advertising.Expenditure`.

```{r}
p.hist.Expenditure <-
  ggplot(data = df.advert,
         mapping = aes(x = Advertising.Expenditure)) +
  geom_histogram() +
  labs(y = "Number of Cities",
       x = "Advertising Expenditure",
       title = "Distribution of Advertising Expenditure Across 200 Different Cities")
p.hist.Expenditure

p.boxplot.Expenditure <-
  ggplot(data = df.advert,
         mapping = aes(x = Advertising.Expenditure)) +
  stat_boxplot(geom = "errorbar", width = 0.2) +
  geom_boxplot() +
  labs(x = "Advertising Expenditure") +
  theme(axis.text.y = element_blank())
p.boxplot.Expenditure
```

## Slide 14

In addition to checking the distribution using histogram or boxplot, we can also compute the skewness of each distribution using the `skewness()` function. The input to this function is the variable we are interested in, in this case, `Sales.Revenue` and `Advertising.Expenditure`.

```{r}
skewness(df.advert$Sales.Revenue)
skewness(df.advert$Advertising.Expenditure)
```

## Slide 15

We now need to check if the relationship between the predictor and response variables is linear. We will plot a scatter plot to check this. Here are the steps:

1.  As before, we use the `ggplot()` function for this plot. The dataset in `df.advert` is passed to the input parameter `data`.
2.  Add the `geom` layer to the plot. Since this is a scatter plot, we will add the `geom_point()` function.
3.  Update the axis labels for the x axis using the `labs()` function.

```{r}
p.scatter.slr <- ggplot(data = df.advert,
       mapping = aes(y = Sales.Revenue,
                     x = Advertising.Expenditure)) +
  geom_point() +
  labs(y = "Sales Revenue",
       x = "Advertising Expenditure",
       title = "Sales Revenue vs Advertising Expenditure")
p.scatter.slr 
```

We can also compute the correlation coefficient between the two variables. We use the `cor()` function, and provide the two variables of interest, in this case, `Sales.Revenue` and `Advertising.Expenditure`, as inputs.

```{r}
cor(df.advert$Sales.Revenue, df.advert$Advertising.Expenditure)
```

## Slide 18

We will now randomly split the data into train and test datasets. We will use the `sample()` function to randomly sample observations from the dataset. But before doing this, the `set.seed()` function is used to set a seed in R, such that the code is *reproducible*. In other words, every time we run this code, the same set of observations are sampled. You can choose any integer to set the seed.

```{r}
set.seed(10)
```

Next, we use the `sample()` function. We pass the total number of rows in the dataset for the input parameter `x`. This represents the range of values from which to generate a random set of observations. The `size` parameter represents the size of the random set. Here, we want 80% of the dataset to be in the train dataset. Therefore we set the size to be 80% times the total number of observations in the dataset. Note that this random sample contains the positions of the corresponding observations in the data frame, also know as *index*.

```{r}
index <- sample(x = nrow(df.advert), size = nrow(df.advert)*.8)
```

Next, we sort the index values in the increasing order, using the `sort()` function, so it is convenient to view in comparison to the original dataset.

```{r}
index <- sort(index)
```

Now we are ready to generate the train and test set. We assign the randomly sampled 80% of the data in `index` to `train` and the rest of the dataset to `test`. Note the use of the minus sign, `-index`, to select observation whose positions are not in `index`.

```{r}
train <- df.advert[index, ]
test <- df.advert[-index, ]
```

## Slide 20

Here, we view a scatter plot, that we have already generated and saved to the variable, `p.scatter.slr`.

```{r}
p.scatter.slr
```

## Slide 22

Let us now fit a simple linear regression model to our dataset, using the `lm()` function, which requires these inputs:

-   Formula: this is a description of the model to be fitted. It takes the form `Y ~ X` where `Y` is the dependent variable and `X` is the independent variable. Our response or dependent variable is `Sales.Revenue`. We have a single predictor or independent variable, `Advertising.Expenditure`.

-   Data: We provide the train dataset `train` as input, as we want the model to be trained on this data.

The model output is stored in `lm.slr`.

```{r}
lm.slr <- lm(formula = Sales.Revenue ~ Advertising.Expenditure,
             data = train)
```

We can now extract the *residuals* of the model using the `resid()` function.

```{r}
residuals <- resid(lm.slr)
head(residuals)
```

## Slide 23

To check that the residuals are normally distributed, we can plot the histogram of residuals. Note that we don't specify the `data` in the `ggplot()` function, instead, we only specify the `mapping` parameter. Since we need a histogram of the residuals, the residuals for the model, `lm.slr` are extracted using the `resid()` function as before, and assigned to the `x` aesthetic in the `aes()` function.

```{r}
ggplot(mapping = aes(x = resid(lm.slr))) +
  geom_histogram() +
  labs(x = "Residuals",
       y = "Number of Cities",
       title = "Histogram of Residuals for Advertising Expenditure")
```

## Slide 24

To check that the residuals are evenly scattered, we generate the scatter plot of the residuals vs. the predicted sales revenue. Note that there are a few changes in the scatter plot:

-   Only the `mapping` parameter is specified here. The predicted sales revenue is extracted using the `fitted()` function and the residuals are extracted using the `resid()` function.

-   We use `geom_abline()` function to draw a red horizontal line with zero slope. The residuals need to be evenly scattered around this line.

```{r}
ggplot(mapping = aes(x = fitted(lm.slr), y = resid(lm.slr))) +
  geom_point() +
  geom_abline(slope = 0,
              size = 0.5,
              colour = "red") +
  labs(x = "Fitted Values",
       y = "Residuals",
       title = "Residual plot for Advertising Expenditure")
```

## Slide 25

We can generate the regression line in the scatter plot between `Sales.Revenue` and `Advertising.Expenditure` without explicitly modelling the linear regression using the `lm()` function. We start with the scatter plot as before, but use the train data as the `data` parameter. We add a new `geom` function called `geom_smooth()`. In this function, we specify the `method` as "lm", and this produces the simple linear regression line.

```{r}
p.scatterline.slr <- 
  ggplot(data = train,
       mapping = aes(y = Sales.Revenue,
                     x = Advertising.Expenditure)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(y = "Sales Revenue",
       x = "Advertising Expenditure",
       title = "Sales Revenue vs Advertising Expenditure") 
p.scatterline.slr
```

## Slide 30-34

We now consider the model summary. This is generated using the `summary()` function.

```{r}
summary(lm.slr)
```

## Slide 38

To compute the confidence intervals for the parameters in the fitted model, `lm.slr` we use the `confint()` function. Since we need the 95% confidence intervals, the `level` parameter is set to 0.95.

```{r}
confint(lm.slr, level = 0.95)
```

## Slide 40

The `predict()` function is used to make predictions using the fitted model. Let us use the model, `lm.slr` to predict the sales revenue, when the advertising expenditure is 200. We provide this as input in the form of a new data frame that contains the predictor, `Advertising.Expenditure` to the `newdata` parameter in the `predict()` function.

```{r}
newdata <- data.frame(Advertising.Expenditure = 200)
predict(lm.slr, newdata = newdata)
```

We can modify the `predict()` function to also compute the confident interval by setting the `interval` parameter to "confidence".

```{r}
predict(lm.slr, newdata = newdata, interval ="confidence")
```

We can also modify the `predict()` function to compute the prediction interval by setting the `interval` parameter to "prediction" instead.

```{r}
predict(lm.slr, newdata = newdata, interval ="prediction")
```

## Slide 42-45

We will now compute the evaluation metrics for the simple linear regression model, `lm.slr`. We define a function, also referred to as *user-defined function* `eval.metrics.linreg` that computes all the metrics of interest: MSE, MAE, RMSE and MAPE. Note that the evaluation metrics is computed for train and test data. We pass in the actual values and the predicted values to the function as inputs `actual` and `predicted`.

```{r}
eval.metrics.linreg <- function(actual, predicted) {
  residual <- actual - predicted
  mse <- mean(residual ^ 2)
  mae <- mean(abs(residual))
  rmse <-  sqrt(mse)
  mape <- mean(abs(residual / actual)) * 100
  
  data.frame(
    MSE = mse,
    MAE = mae,
    RMSE = rmse,
    MAPE = mape
  )
}
```

To compute the evaluation metrics for the train dataset, we first assign the `actual` and `predicted` values associated with the `train` data. Note how the `predict()` function is used on the entire data frame `train` .

```{r}
actual <- train$Sales.Revenue
predicted <- predict(lm.slr, newdata = train)
eval.metrics.linreg(actual, predicted)
```

We can compute the evaluation metrics for the test data in a similar manner.

```{r}
actual <- test$Sales.Revenue
predicted <- predict(lm.slr, newdata = test)
eval.metrics.linreg(actual, predicted)
```

## Slide 46

```{r}
p.scatterline.slr
```
